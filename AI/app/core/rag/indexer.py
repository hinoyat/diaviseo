import pickle
import logging
import requests, tempfile
from pathlib import Path
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from app.config.log import logging_check
from app.config.settings import get_settings

logging_check()

'''
1ë‹¨ê³„ : ë¬¸ì„œ ë¡œë“œ(Load Documents)
2ë‹¨ê³„ : ë¬¸ì„œ ë¶„í• (split Documents)
3ë‹¨ê³„ : ì„ë² ë”©(embedding)
4ë‹¨ê³„ : DB ìƒì„± ë° ì €ì¥
'''

settings = get_settings()
def build_index():
    logging.info("ğŸ“£ build_index() ì‹œì‘")
    # 1) í”„ë¡œì íŠ¸ ë£¨íŠ¸/data
    # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ì—ì„œ ì‹œì‘
    p = Path(__file__).resolve()
    # 'AI'ë¼ëŠ” ì´ë¦„ì˜ ë””ë ‰í„°ë¦¬ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ parentë¡œ ì˜¬ë¼ê°
    while p.name != "AI":
        p = p.parent
    project_root = p


    data_dir = project_root / "data"
    data_dir.mkdir(parents=True, exist_ok=True)
    logging.info(f"Data directory: {data_dir}")

    # 2) ì´ë¯¸ ìƒ‰ì¸ ë¼ ìˆìœ¼ë©´ ìŠ¤í‚µ
    pkl_path  = data_dir / "nutrition_split_documents.pkl"
    faiss_path = data_dir / "nutrition_faiss_index"

    if pkl_path.exists() and (faiss_path / "index.faiss").exists() and (faiss_path / "index.pkl").exists():
        logging.info("â—ï¸ ì¸ë±ìŠ¤ ì´ë¯¸ ì¡´ì¬â€”ì¬ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    pdf_url = settings.pdf_url
    # 1) PDF ë‹¤ìš´ë¡œë“œí•´ì„œ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    resp = requests.get(pdf_url)
    resp.raise_for_status()
    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tf:
        tf.write(resp.content)
        pdf_path = tf.name

    # 2) PyMuPDFLoaderë¡œ í˜ì´ì§€ë³„ Document ë¡œë“œ
    loader = PyMuPDFLoader(pdf_path)
    pages  = loader.load()

    logging.info(f"í˜ì´ì§€(Document) ìˆ˜: {len(pages)}")

    # 3) ì²­í¬ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_documents = text_splitter.split_documents(pages)
    with open(pkl_path, "wb") as f:
        pickle.dump(split_documents, f)    # type: ignore
    logging.info("split_documents.pkl ìƒì„± ì™„ë£Œ")

    # 4) ì„ë² ë”©
    embeddings = HuggingFaceEmbeddings(
        model_name="dragonkue/snowflake-arctic-embed-l-v2.0-ko"
    )

    # 5) FAISS
    vectorstore = FAISS.from_documents(split_documents, embeddings)

    # 6) ë¡œì»¬ì— ì €ì¥
    vectorstore.save_local("/tmp/nutrition_faiss_index")
    logging.info("nutrition_faiss_index.idx ìƒì„± ì™„ë£Œ : {faiss_path}")